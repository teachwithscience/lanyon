---
title: "Factor Analysis"
author: "null"
date: '2017-09-15'
slug: factor-analysis
tags: []
categories: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
options(scipen=999)
```

```{r}
Movies <- read.csv("https://raw.githubusercontent.com/centerandspread/centerandspread.github.io/master/data/USAMovies.csv")
library(scales)
library(ggplot2)
library(psych)
```

## Dimension Reduction / PCA
Designed to reduce the number of predictor variables (reducing multicollinearity), essentially transforms the dataset to find a reduced set of variables that still explain most of the variance.

This is an unsupervised learning method. It seeks to use mathematical variance to cluster variables into common factors, without guidance from the researcher.

For low loading (low dimensional) variables (those with low correlation with the outcome variable).

The model aims to capture as much information as possible, with high explained variance in each component. The first component has the highest amount of variance explained, followed by the second, third and so on.

"The objective of PCA is to maximize variance while minimizing covariance."

## Unsupervised Learning Models
PCA
K-means (nearest neighbors?)
GLRM (generalized low rank models)

#### Transformation
You might want to transform the data before putting it into the model. Options: Standardize, Normalize, Descale, Demean, nothing.

Why? Because the original predictors might be on different scales (feet, meters, pints, gallons, etc.). Need to put them all on a similar scale if looking at collinearity / mutlicollinearity.


#### Formula
Several PCA formulas exist:
GramSVD, Power, Randomized, GLRM (generalized low-rank model)


## Factor Analysis

Sometimes variability can be explained by something you can't seem to categorize. It seems to be hidden in the data. Such "latent variables" are what an exploratory factor analysis (EFA) looks for using mathematical computations.

Explaining variance by narrowing it down, finding clusters of similar responses.

PCA is a normalized linear combination of the predictors in a data set. It seeks to explain the total variance with a fewer number of total variables (looks to cluster variables that explain similar variance).

ONLY load predictor variables. Do not include outcome variable.

First component found, then second should be orthogonal.

Called orthogonal because looks for noncorrelated variables. If the two components are uncorrelated, their directions should be orthogonal (perpendicular).

All of the principal components found next follow the same pattern; they seek to capture remaining variation while also remaining uncorrelated with previous components.

Must use ONLY numeric data. Cannot use categorical (must be dummy coded first). Use one-hot encoding (dummy).

#### Standard Example-
In R, we can use prcomp() to conduce a PCA. This function centers the variables to have means equal to zero. Transformation to a normal scale can be done with scale=T (makes variables have SD equal to one).

After running, check the factor loadings of the variables.

> prin_comp$rotation

Total number of factors with loadings should be minimum of (n-1), so there might be a lot!

Can get a visual of the components:
> biplot(prin_comp, scale = 0)

Can also make scree plot to look for total number of components to keep, or make a cumulative scree plot.
             


#### Example
Hollywood blockbusters are expensive affairs. Budgets are often in the hundreds of millions of dollars, especially when big name actors/actresses and directors are involved. You could probably name a few of the top directors and expect that they would likely receive very large budgets, while those relatively new might have to make magic happen with a much smaller budget. 

Let's take a look at variability in budgets and total number of directors for films in dataset to see if there is variability in the first place to try to describe / explain.

```{r}
dollar_format()(mean(Movies$budget))
summary(Movies$budget)
```

Whoa, so the lowest budget was a meager $1,100, and the largest was $300,000,000. That's quite a range!

Okay, but what about the visual distribution of budgets?

```{r}
ggplot(Movies, aes(x=budget)) + geom_histogram(binwidth=50000000, colour="black", alpha=0.4, aes(y=..count.., fill=..density..)) + scale_x_continuous(label=dollar, breaks=c(0,50000000,100000000,150000000,200000000,250000000,300000000)) + theme(axis.text.x=element_text(angle = -90, hjust = 0))
```

Now, how many directors are there for the 3,237 movies we have in our dataset?

```{r}
unique(Movies$director_name)
```

Whoa, that was a lot. 1,511 different directors. Yikes. Let's see if we can find some similarities between them (group or cluster them) using exploratory factor analysis.

Can we group directors in Hollywood by the type of budgets they receive to make their art? Are there clusters of directors that might help to explain the variability in budgets given to movies?

## Step One - Choose Number of Factors
Ideally, you will have a guiding theory as to how many factors (clusters) are at work in your variable







*3,237 movies from the USA (reported budgets otherwise not in standard currency) from _ to _ on IMDB.
